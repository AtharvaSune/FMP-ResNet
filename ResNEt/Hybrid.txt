Model Summary 

Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 30, 30, 32)   896         input_1[0][0]                    
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 28, 28, 64)   18496       conv2d[0][0]                     
__________________________________________________________________________________________________
fractional_pooling2d (Fractiona (None, 9, 9, 64)     0           conv2d_1[0][0]                   
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 9, 9, 64)     36928       fractional_pooling2d[0][0]       
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 9, 9, 64)     256         conv2d_2[0][0]                   
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 9, 9, 64)     36928       batch_normalization[0][0]        
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 9, 9, 64)     256         conv2d_3[0][0]                   
__________________________________________________________________________________________________
add (Add)                       (None, 9, 9, 64)     0           batch_normalization_1[0][0]      
                                                                 fractional_pooling2d[0][0]       
__________________________________________________________________________________________________
activation (Activation)         (None, 9, 9, 64)     0           add[0][0]                        
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 9, 9, 64)     36928       activation[0][0]                 
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 9, 9, 64)     256         conv2d_4[0][0]                   
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 9, 9, 64)     36928       batch_normalization_2[0][0]      
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 9, 9, 64)     256         conv2d_5[0][0]                   
__________________________________________________________________________________________________
add_1 (Add)                     (None, 9, 9, 64)     0           batch_normalization_3[0][0]      
                                                                 activation[0][0]                 
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 9, 9, 64)     0           add_1[0][0]                      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 9, 9, 64)     36928       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 9, 9, 64)     256         conv2d_6[0][0]                   
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 9, 9, 64)     36928       batch_normalization_4[0][0]      
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 9, 9, 64)     256         conv2d_7[0][0]                   
__________________________________________________________________________________________________
add_2 (Add)                     (None, 9, 9, 64)     0           batch_normalization_5[0][0]      
                                                                 activation_1[0][0]               
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 9, 9, 64)     0           add_2[0][0]                      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 9, 9, 64)     36928       activation_2[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 9, 9, 64)     256         conv2d_8[0][0]                   
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 9, 9, 64)     36928       batch_normalization_6[0][0]      
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 9, 9, 64)     256         conv2d_9[0][0]                   
__________________________________________________________________________________________________
add_3 (Add)                     (None, 9, 9, 64)     0           batch_normalization_7[0][0]      
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 9, 9, 64)     0           add_3[0][0]                      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 9, 9, 64)     36928       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 9, 9, 64)     256         conv2d_10[0][0]                  
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 9, 9, 64)     36928       batch_normalization_8[0][0]      
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 9, 9, 64)     256         conv2d_11[0][0]                  
__________________________________________________________________________________________________
add_4 (Add)                     (None, 9, 9, 64)     0           batch_normalization_9[0][0]      
                                                                 activation_3[0][0]               
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 9, 9, 64)     0           add_4[0][0]                      
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 9, 9, 64)     36928       activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 9, 9, 64)     256         conv2d_12[0][0]                  
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 9, 9, 64)     36928       batch_normalization_10[0][0]     
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 9, 9, 64)     256         conv2d_13[0][0]                  
__________________________________________________________________________________________________
add_5 (Add)                     (None, 9, 9, 64)     0           batch_normalization_11[0][0]     
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 9, 9, 64)     0           add_5[0][0]                      
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 9, 9, 64)     36928       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 9, 9, 64)     256         conv2d_14[0][0]                  
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 9, 9, 64)     36928       batch_normalization_12[0][0]     
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 9, 9, 64)     256         conv2d_15[0][0]                  
__________________________________________________________________________________________________
add_6 (Add)                     (None, 9, 9, 64)     0           batch_normalization_13[0][0]     
                                                                 activation_5[0][0]               
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 9, 9, 64)     0           add_6[0][0]                      
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 9, 9, 64)     36928       activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 9, 9, 64)     256         conv2d_16[0][0]                  
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 9, 9, 64)     36928       batch_normalization_14[0][0]     
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 9, 9, 64)     256         conv2d_17[0][0]                  
__________________________________________________________________________________________________
add_7 (Add)                     (None, 9, 9, 64)     0           batch_normalization_15[0][0]     
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 9, 9, 64)     0           add_7[0][0]                      
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 9, 9, 64)     36928       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 9, 9, 64)     256         conv2d_18[0][0]                  
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 9, 9, 64)     36928       batch_normalization_16[0][0]     
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 9, 9, 64)     256         conv2d_19[0][0]                  
__________________________________________________________________________________________________
add_8 (Add)                     (None, 9, 9, 64)     0           batch_normalization_17[0][0]     
                                                                 activation_7[0][0]               
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 9, 9, 64)     0           add_8[0][0]                      
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 9, 9, 64)     36928       activation_8[0][0]               
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 9, 9, 64)     256         conv2d_20[0][0]                  
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 9, 9, 64)     36928       batch_normalization_18[0][0]     
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 9, 9, 64)     256         conv2d_21[0][0]                  
__________________________________________________________________________________________________
add_9 (Add)                     (None, 9, 9, 64)     0           batch_normalization_19[0][0]     
                                                                 activation_8[0][0]               
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 9, 9, 64)     0           add_9[0][0]                      
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 7, 7, 64)     36928       activation_9[0][0]               
__________________________________________________________________________________________________
fractional_pooling2d_1 (Fractio (None, 2, 2, 64)     0           conv2d_22[0][0]                  
__________________________________________________________________________________________________
flatten (Flatten)               (None, 256)          0           fractional_pooling2d_1[0][0]     
__________________________________________________________________________________________________
dense (Dense)                   (None, 256)          65792       flatten[0][0]                    
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 10)           2570        dense[0][0]                      
==================================================================================================
Total params: 868,362
Trainable params: 865,802
Non-trainable params: 2,560

Training Details

Epoch 1/30
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
194/195 [============================>.] - ETA: 0s - loss: 2.0074 - acc: 0.2991
Epoch 00001: val_loss improved from inf to 2.78593, saving model to Model.hdf5
195/195 [==============================] - 22s 115ms/step - loss: 2.0057 - acc: 0.2991 - val_loss: 2.7859 - val_acc: 0.1287
Epoch 2/30
194/195 [============================>.] - ETA: 0s - loss: 1.5709 - acc: 0.4253
Epoch 00002: val_loss improved from 2.78593 to 2.04751, saving model to Model.hdf5
195/195 [==============================] - 12s 61ms/step - loss: 1.5700 - acc: 0.4258 - val_loss: 2.0475 - val_acc: 0.2735
Epoch 3/30
194/195 [============================>.] - ETA: 0s - loss: 1.4282 - acc: 0.4863
Epoch 00003: val_loss improved from 2.04751 to 1.63541, saving model to Model.hdf5
195/195 [==============================] - 14s 70ms/step - loss: 1.4279 - acc: 0.4863 - val_loss: 1.6354 - val_acc: 0.4264
Epoch 4/30
194/195 [============================>.] - ETA: 0s - loss: 1.2816 - acc: 0.5404
Epoch 00004: val_loss improved from 1.63541 to 1.53440, saving model to Model.hdf5
195/195 [==============================] - 12s 63ms/step - loss: 1.2816 - acc: 0.5405 - val_loss: 1.5344 - val_acc: 0.4506
Epoch 5/30
194/195 [============================>.] - ETA: 0s - loss: 1.1690 - acc: 0.5844
Epoch 00005: val_loss did not improve from 1.53440
195/195 [==============================] - 12s 62ms/step - loss: 1.1673 - acc: 0.5849 - val_loss: 1.5565 - val_acc: 0.4649
Epoch 6/30
194/195 [============================>.] - ETA: 0s - loss: 1.0974 - acc: 0.6136
Epoch 00006: val_loss improved from 1.53440 to 1.41194, saving model to Model.hdf5
195/195 [==============================] - 12s 63ms/step - loss: 1.0977 - acc: 0.6136 - val_loss: 1.4119 - val_acc: 0.5149
Epoch 7/30
194/195 [============================>.] - ETA: 0s - loss: 1.0411 - acc: 0.6293
Epoch 00007: val_loss did not improve from 1.41194
195/195 [==============================] - 12s 63ms/step - loss: 1.0415 - acc: 0.6291 - val_loss: 1.5150 - val_acc: 0.5284
Epoch 8/30
194/195 [============================>.] - ETA: 0s - loss: 1.0055 - acc: 0.6457
Epoch 00008: val_loss improved from 1.41194 to 1.27333, saving model to Model.hdf5
195/195 [==============================] - 12s 61ms/step - loss: 1.0043 - acc: 0.6462 - val_loss: 1.2733 - val_acc: 0.5588
Epoch 9/30
194/195 [============================>.] - ETA: 0s - loss: 0.9320 - acc: 0.6762
Epoch 00009: val_loss improved from 1.27333 to 1.10468, saving model to Model.hdf5
195/195 [==============================] - 12s 63ms/step - loss: 0.9314 - acc: 0.6763 - val_loss: 1.1047 - val_acc: 0.6108
Epoch 10/30
194/195 [============================>.] - ETA: 0s - loss: 0.8919 - acc: 0.6839
Epoch 00010: val_loss did not improve from 1.10468
195/195 [==============================] - 12s 62ms/step - loss: 0.8915 - acc: 0.6839 - val_loss: 1.1187 - val_acc: 0.6214
Epoch 11/30
194/195 [============================>.] - ETA: 0s - loss: 0.8585 - acc: 0.6942
Epoch 00011: val_loss improved from 1.10468 to 0.92504, saving model to Model.hdf5
195/195 [==============================] - 12s 62ms/step - loss: 0.8589 - acc: 0.6941 - val_loss: 0.9250 - val_acc: 0.6793
Epoch 12/30
194/195 [============================>.] - ETA: 0s - loss: 0.8520 - acc: 0.7043
Epoch 00012: val_loss did not improve from 0.92504
195/195 [==============================] - 12s 60ms/step - loss: 0.8515 - acc: 0.7047 - val_loss: 0.9510 - val_acc: 0.6755
Epoch 13/30
194/195 [============================>.] - ETA: 0s - loss: 0.7906 - acc: 0.7295
Epoch 00013: val_loss did not improve from 0.92504
195/195 [==============================] - 12s 62ms/step - loss: 0.7907 - acc: 0.7292 - val_loss: 1.1572 - val_acc: 0.6176
Epoch 14/30
194/195 [============================>.] - ETA: 0s - loss: 0.7832 - acc: 0.7266
Epoch 00014: val_loss did not improve from 0.92504
195/195 [==============================] - 12s 62ms/step - loss: 0.7834 - acc: 0.7265 - val_loss: 1.0660 - val_acc: 0.6429
Epoch 15/30
194/195 [============================>.] - ETA: 0s - loss: 0.7621 - acc: 0.7365
Epoch 00015: val_loss did not improve from 0.92504
195/195 [==============================] - 12s 60ms/step - loss: 0.7616 - acc: 0.7364 - val_loss: 0.9958 - val_acc: 0.6668
Epoch 16/30
194/195 [============================>.] - ETA: 0s - loss: 0.7344 - acc: 0.7432
Epoch 00016: val_loss did not improve from 0.92504
195/195 [==============================] - 14s 70ms/step - loss: 0.7338 - acc: 0.7434 - val_loss: 1.0019 - val_acc: 0.6611
Epoch 17/30
193/195 [============================>.] - ETA: 0s - loss: 0.6906 - acc: 0.7582
Epoch 00017: val_loss improved from 0.92504 to 0.84510, saving model to Model.hdf5
195/195 [==============================] - 12s 61ms/step - loss: 0.6904 - acc: 0.7583 - val_loss: 0.8451 - val_acc: 0.7125
Epoch 18/30
194/195 [============================>.] - ETA: 0s - loss: 0.6825 - acc: 0.7643
Epoch 00018: val_loss did not improve from 0.84510
195/195 [==============================] - 12s 60ms/step - loss: 0.6832 - acc: 0.7639 - val_loss: 0.8732 - val_acc: 0.7107
Epoch 19/30
193/195 [============================>.] - ETA: 0s - loss: 0.6879 - acc: 0.7571
Epoch 00019: val_loss improved from 0.84510 to 0.76086, saving model to Model.hdf5
195/195 [==============================] - 12s 63ms/step - loss: 0.6876 - acc: 0.7571 - val_loss: 0.7609 - val_acc: 0.7401
Epoch 20/30
193/195 [============================>.] - ETA: 0s - loss: 0.6787 - acc: 0.7679
Epoch 00020: val_loss did not improve from 0.76086
195/195 [==============================] - 12s 62ms/step - loss: 0.6776 - acc: 0.7683 - val_loss: 1.2382 - val_acc: 0.6101
Epoch 21/30
194/195 [============================>.] - ETA: 0s - loss: 0.6262 - acc: 0.7836- ETA: 1s - loss: 0.627
Epoch 00021: val_loss did not improve from 0.76086
195/195 [==============================] - 12s 61ms/step - loss: 0.6255 - acc: 0.7839 - val_loss: 0.8228 - val_acc: 0.7233
Epoch 22/30
194/195 [============================>.] - ETA: 0s - loss: 0.6132 - acc: 0.7844
Epoch 00022: val_loss did not improve from 0.76086
195/195 [==============================] - 12s 63ms/step - loss: 0.6133 - acc: 0.7847 - val_loss: 0.8356 - val_acc: 0.7161
Epoch 23/30
194/195 [============================>.] - ETA: 0s - loss: 0.6264 - acc: 0.7824
Epoch 00023: val_loss did not improve from 0.76086
195/195 [==============================] - 12s 60ms/step - loss: 0.6264 - acc: 0.7823 - val_loss: 0.8726 - val_acc: 0.7101
Epoch 24/30
194/195 [============================>.] - ETA: 0s - loss: 0.6259 - acc: 0.7865
Epoch 00024: val_loss did not improve from 0.76086
195/195 [==============================] - 12s 60ms/step - loss: 0.6250 - acc: 0.7869 - val_loss: 0.7740 - val_acc: 0.7379
Epoch 25/30
193/195 [============================>.] - ETA: 0s - loss: 0.5519 - acc: 0.8080
Epoch 00025: val_loss improved from 0.76086 to 0.75890, saving model to Model.hdf5
195/195 [==============================] - 12s 63ms/step - loss: 0.5522 - acc: 0.8080 - val_loss: 0.7589 - val_acc: 0.7444
Epoch 26/30
194/195 [============================>.] - ETA: 0s - loss: 0.5666 - acc: 0.8031
Epoch 00026: val_loss improved from 0.75890 to 0.74302, saving model to Model.hdf5
195/195 [==============================] - 12s 62ms/step - loss: 0.5661 - acc: 0.8033 - val_loss: 0.7430 - val_acc: 0.7500
Epoch 27/30
194/195 [============================>.] - ETA: 0s - loss: 0.5635 - acc: 0.8080
Epoch 00027: val_loss did not improve from 0.74302
195/195 [==============================] - 12s 62ms/step - loss: 0.5648 - acc: 0.8076 - val_loss: 0.9474 - val_acc: 0.6985
Epoch 28/30
193/195 [============================>.] - ETA: 0s - loss: 0.5752 - acc: 0.8023
Epoch 00028: val_loss improved from 0.74302 to 0.73759, saving model to Model.hdf5
195/195 [==============================] - 12s 61ms/step - loss: 0.5756 - acc: 0.8020 - val_loss: 0.7376 - val_acc: 0.7557
Epoch 29/30
193/195 [============================>.] - ETA: 0s - loss: 0.5014 - acc: 0.8237
Epoch 00029: val_loss did not improve from 0.73759
195/195 [==============================] - 12s 60ms/step - loss: 0.5022 - acc: 0.8236 - val_loss: 0.7810 - val_acc: 0.7383
Epoch 30/30
194/195 [============================>.] - ETA: 0s - loss: 0.5349 - acc: 0.8140
Epoch 00030: val_loss improved from 0.73759 to 0.73398, saving model to Model.hdf5
195/195 [==============================] - 12s 59ms/step - loss: 0.5352 - acc: 0.8142 - val_loss: 0.7340 - val_acc: 0.7558
