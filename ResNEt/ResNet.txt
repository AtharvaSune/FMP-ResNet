Model Summary

Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_4 (InputLayer)            [(None, 32, 32, 3)]  0                                            
__________________________________________________________________________________________________
conv2d_69 (Conv2D)              (None, 30, 30, 32)   896         input_4[0][0]                    
__________________________________________________________________________________________________
conv2d_70 (Conv2D)              (None, 28, 28, 64)   18496       conv2d_69[0][0]                  
__________________________________________________________________________________________________
max_pooling2d_6 (MaxPooling2D)  (None, 9, 9, 64)     0           conv2d_70[0][0]                  
__________________________________________________________________________________________________
conv2d_71 (Conv2D)              (None, 9, 9, 64)     36928       max_pooling2d_6[0][0]            
__________________________________________________________________________________________________
batch_normalization_60 (BatchNo (None, 9, 9, 64)     256         conv2d_71[0][0]                  
__________________________________________________________________________________________________
conv2d_72 (Conv2D)              (None, 9, 9, 64)     36928       batch_normalization_60[0][0]     
__________________________________________________________________________________________________
batch_normalization_61 (BatchNo (None, 9, 9, 64)     256         conv2d_72[0][0]                  
__________________________________________________________________________________________________
add_30 (Add)                    (None, 9, 9, 64)     0           batch_normalization_61[0][0]     
                                                                 max_pooling2d_6[0][0]            
__________________________________________________________________________________________________
activation_30 (Activation)      (None, 9, 9, 64)     0           add_30[0][0]                     
__________________________________________________________________________________________________
conv2d_73 (Conv2D)              (None, 9, 9, 64)     36928       activation_30[0][0]              
__________________________________________________________________________________________________
batch_normalization_62 (BatchNo (None, 9, 9, 64)     256         conv2d_73[0][0]                  
__________________________________________________________________________________________________
conv2d_74 (Conv2D)              (None, 9, 9, 64)     36928       batch_normalization_62[0][0]     
__________________________________________________________________________________________________
batch_normalization_63 (BatchNo (None, 9, 9, 64)     256         conv2d_74[0][0]                  
__________________________________________________________________________________________________
add_31 (Add)                    (None, 9, 9, 64)     0           batch_normalization_63[0][0]     
                                                                 activation_30[0][0]              
__________________________________________________________________________________________________
activation_31 (Activation)      (None, 9, 9, 64)     0           add_31[0][0]                     
__________________________________________________________________________________________________
conv2d_75 (Conv2D)              (None, 9, 9, 64)     36928       activation_31[0][0]              
__________________________________________________________________________________________________
batch_normalization_64 (BatchNo (None, 9, 9, 64)     256         conv2d_75[0][0]                  
__________________________________________________________________________________________________
conv2d_76 (Conv2D)              (None, 9, 9, 64)     36928       batch_normalization_64[0][0]     
__________________________________________________________________________________________________
batch_normalization_65 (BatchNo (None, 9, 9, 64)     256         conv2d_76[0][0]                  
__________________________________________________________________________________________________
add_32 (Add)                    (None, 9, 9, 64)     0           batch_normalization_65[0][0]     
                                                                 activation_31[0][0]              
__________________________________________________________________________________________________
activation_32 (Activation)      (None, 9, 9, 64)     0           add_32[0][0]                     
__________________________________________________________________________________________________
conv2d_77 (Conv2D)              (None, 9, 9, 64)     36928       activation_32[0][0]              
__________________________________________________________________________________________________
batch_normalization_66 (BatchNo (None, 9, 9, 64)     256         conv2d_77[0][0]                  
__________________________________________________________________________________________________
conv2d_78 (Conv2D)              (None, 9, 9, 64)     36928       batch_normalization_66[0][0]     
__________________________________________________________________________________________________
batch_normalization_67 (BatchNo (None, 9, 9, 64)     256         conv2d_78[0][0]                  
__________________________________________________________________________________________________
add_33 (Add)                    (None, 9, 9, 64)     0           batch_normalization_67[0][0]     
                                                                 activation_32[0][0]              
__________________________________________________________________________________________________
activation_33 (Activation)      (None, 9, 9, 64)     0           add_33[0][0]                     
__________________________________________________________________________________________________
conv2d_79 (Conv2D)              (None, 9, 9, 64)     36928       activation_33[0][0]              
__________________________________________________________________________________________________
batch_normalization_68 (BatchNo (None, 9, 9, 64)     256         conv2d_79[0][0]                  
__________________________________________________________________________________________________
conv2d_80 (Conv2D)              (None, 9, 9, 64)     36928       batch_normalization_68[0][0]     
__________________________________________________________________________________________________
batch_normalization_69 (BatchNo (None, 9, 9, 64)     256         conv2d_80[0][0]                  
__________________________________________________________________________________________________
add_34 (Add)                    (None, 9, 9, 64)     0           batch_normalization_69[0][0]     
                                                                 activation_33[0][0]              
__________________________________________________________________________________________________
activation_34 (Activation)      (None, 9, 9, 64)     0           add_34[0][0]                     
__________________________________________________________________________________________________
conv2d_81 (Conv2D)              (None, 9, 9, 64)     36928       activation_34[0][0]              
__________________________________________________________________________________________________
batch_normalization_70 (BatchNo (None, 9, 9, 64)     256         conv2d_81[0][0]                  
__________________________________________________________________________________________________
conv2d_82 (Conv2D)              (None, 9, 9, 64)     36928       batch_normalization_70[0][0]     
__________________________________________________________________________________________________
batch_normalization_71 (BatchNo (None, 9, 9, 64)     256         conv2d_82[0][0]                  
__________________________________________________________________________________________________
add_35 (Add)                    (None, 9, 9, 64)     0           batch_normalization_71[0][0]     
                                                                 activation_34[0][0]              
__________________________________________________________________________________________________
activation_35 (Activation)      (None, 9, 9, 64)     0           add_35[0][0]                     
__________________________________________________________________________________________________
conv2d_83 (Conv2D)              (None, 9, 9, 64)     36928       activation_35[0][0]              
__________________________________________________________________________________________________
batch_normalization_72 (BatchNo (None, 9, 9, 64)     256         conv2d_83[0][0]                  
__________________________________________________________________________________________________
conv2d_84 (Conv2D)              (None, 9, 9, 64)     36928       batch_normalization_72[0][0]     
__________________________________________________________________________________________________
batch_normalization_73 (BatchNo (None, 9, 9, 64)     256         conv2d_84[0][0]                  
__________________________________________________________________________________________________
add_36 (Add)                    (None, 9, 9, 64)     0           batch_normalization_73[0][0]     
                                                                 activation_35[0][0]              
__________________________________________________________________________________________________
activation_36 (Activation)      (None, 9, 9, 64)     0           add_36[0][0]                     
__________________________________________________________________________________________________
conv2d_85 (Conv2D)              (None, 9, 9, 64)     36928       activation_36[0][0]              
__________________________________________________________________________________________________
batch_normalization_74 (BatchNo (None, 9, 9, 64)     256         conv2d_85[0][0]                  
__________________________________________________________________________________________________
conv2d_86 (Conv2D)              (None, 9, 9, 64)     36928       batch_normalization_74[0][0]     
__________________________________________________________________________________________________
batch_normalization_75 (BatchNo (None, 9, 9, 64)     256         conv2d_86[0][0]                  
__________________________________________________________________________________________________
add_37 (Add)                    (None, 9, 9, 64)     0           batch_normalization_75[0][0]     
                                                                 activation_36[0][0]              
__________________________________________________________________________________________________
activation_37 (Activation)      (None, 9, 9, 64)     0           add_37[0][0]                     
__________________________________________________________________________________________________
conv2d_87 (Conv2D)              (None, 9, 9, 64)     36928       activation_37[0][0]              
__________________________________________________________________________________________________
batch_normalization_76 (BatchNo (None, 9, 9, 64)     256         conv2d_87[0][0]                  
__________________________________________________________________________________________________
conv2d_88 (Conv2D)              (None, 9, 9, 64)     36928       batch_normalization_76[0][0]     
__________________________________________________________________________________________________
batch_normalization_77 (BatchNo (None, 9, 9, 64)     256         conv2d_88[0][0]                  
__________________________________________________________________________________________________
add_38 (Add)                    (None, 9, 9, 64)     0           batch_normalization_77[0][0]     
                                                                 activation_37[0][0]              
__________________________________________________________________________________________________
activation_38 (Activation)      (None, 9, 9, 64)     0           add_38[0][0]                     
__________________________________________________________________________________________________
conv2d_89 (Conv2D)              (None, 9, 9, 64)     36928       activation_38[0][0]              
__________________________________________________________________________________________________
batch_normalization_78 (BatchNo (None, 9, 9, 64)     256         conv2d_89[0][0]                  
__________________________________________________________________________________________________
conv2d_90 (Conv2D)              (None, 9, 9, 64)     36928       batch_normalization_78[0][0]     
__________________________________________________________________________________________________
batch_normalization_79 (BatchNo (None, 9, 9, 64)     256         conv2d_90[0][0]                  
__________________________________________________________________________________________________
add_39 (Add)                    (None, 9, 9, 64)     0           batch_normalization_79[0][0]     
                                                                 activation_38[0][0]              
__________________________________________________________________________________________________
activation_39 (Activation)      (None, 9, 9, 64)     0           add_39[0][0]                     
__________________________________________________________________________________________________
conv2d_91 (Conv2D)              (None, 7, 7, 64)     36928       activation_39[0][0]              
__________________________________________________________________________________________________
max_pooling2d_7 (MaxPooling2D)  (None, 2, 2, 64)     0           conv2d_91[0][0]                  
__________________________________________________________________________________________________
flatten_3 (Flatten)             (None, 256)          0           max_pooling2d_7[0][0]            
__________________________________________________________________________________________________
dense_6 (Dense)                 (None, 256)          65792       flatten_3[0][0]                  
__________________________________________________________________________________________________
dense_7 (Dense)                 (None, 10)           2570        dense_6[0][0]                    
==================================================================================================
Total params: 868,362
Trainable params: 865,802
Non-trainable params: 2,560


Train Summary

Epoch 1/30
193/195 [============================>.] - ETA: 0s - loss: 1.9452 - acc: 0.3059
Epoch 00001: val_loss improved from inf to 2.33582, saving model to Model.hdf5
195/195 [==============================] - 24s 121ms/step - loss: 1.9425 - acc: 0.3073 - val_loss: 2.3358 - val_acc: 0.1163
Epoch 2/30
193/195 [============================>.] - ETA: 0s - loss: 1.5461 - acc: 0.4309
Epoch 00002: val_loss improved from 2.33582 to 2.07772, saving model to Model.hdf5
195/195 [==============================] - 8s 42ms/step - loss: 1.5444 - acc: 0.4313 - val_loss: 2.0777 - val_acc: 0.2580
Epoch 3/30
193/195 [============================>.] - ETA: 0s - loss: 1.3940 - acc: 0.4951
Epoch 00003: val_loss improved from 2.07772 to 1.39476, saving model to Model.hdf5
195/195 [==============================] - 8s 42ms/step - loss: 1.3924 - acc: 0.4955 - val_loss: 1.3948 - val_acc: 0.4884
Epoch 4/30
193/195 [============================>.] - ETA: 0s - loss: 1.2779 - acc: 0.5388
Epoch 00004: val_loss improved from 1.39476 to 1.32269, saving model to Model.hdf5
195/195 [==============================] - 8s 42ms/step - loss: 1.2770 - acc: 0.5390 - val_loss: 1.3227 - val_acc: 0.5323
Epoch 5/30
193/195 [============================>.] - ETA: 0s - loss: 1.1634 - acc: 0.5879
Epoch 00005: val_loss did not improve from 1.32269
195/195 [==============================] - 8s 43ms/step - loss: 1.1639 - acc: 0.5873 - val_loss: 1.4856 - val_acc: 0.4929
Epoch 6/30
193/195 [============================>.] - ETA: 0s - loss: 1.0735 - acc: 0.6195
Epoch 00006: val_loss did not improve from 1.32269
195/195 [==============================] - 8s 42ms/step - loss: 1.0728 - acc: 0.6196 - val_loss: 2.0718 - val_acc: 0.3759
Epoch 7/30
193/195 [============================>.] - ETA: 0s - loss: 1.0250 - acc: 0.6340
Epoch 00007: val_loss did not improve from 1.32269
195/195 [==============================] - 8s 42ms/step - loss: 1.0239 - acc: 0.6340 - val_loss: 1.9518 - val_acc: 0.4237
Epoch 8/30
193/195 [============================>.] - ETA: 0s - loss: 0.9810 - acc: 0.6545
Epoch 00008: val_loss improved from 1.32269 to 1.22509, saving model to Model.hdf5
195/195 [==============================] - 8s 43ms/step - loss: 0.9803 - acc: 0.6547 - val_loss: 1.2251 - val_acc: 0.5836
Epoch 9/30
193/195 [============================>.] - ETA: 0s - loss: 0.9269 - acc: 0.6780
Epoch 00009: val_loss did not improve from 1.22509
195/195 [==============================] - 8s 43ms/step - loss: 0.9251 - acc: 0.6785 - val_loss: 1.7843 - val_acc: 0.4829
Epoch 10/30
193/195 [============================>.] - ETA: 0s - loss: 0.8738 - acc: 0.6943
Epoch 00010: val_loss did not improve from 1.22509
195/195 [==============================] - 8s 42ms/step - loss: 0.8745 - acc: 0.6943 - val_loss: 1.2945 - val_acc: 0.5781
Epoch 11/30
193/195 [============================>.] - ETA: 0s - loss: 0.8749 - acc: 0.6914
Epoch 00011: val_loss improved from 1.22509 to 1.04561, saving model to Model.hdf5
195/195 [==============================] - 9s 44ms/step - loss: 0.8747 - acc: 0.6914 - val_loss: 1.0456 - val_acc: 0.6389
Epoch 12/30
193/195 [============================>.] - ETA: 0s - loss: 0.8246 - acc: 0.7124
Epoch 00012: val_loss did not improve from 1.04561
195/195 [==============================] - 8s 42ms/step - loss: 0.8248 - acc: 0.7121 - val_loss: 1.1108 - val_acc: 0.6227
Epoch 13/30
193/195 [============================>.] - ETA: 0s - loss: 0.7756 - acc: 0.7302
Epoch 00013: val_loss did not improve from 1.04561
195/195 [==============================] - 8s 43ms/step - loss: 0.7746 - acc: 0.7305 - val_loss: 1.0791 - val_acc: 0.6363
Epoch 14/30
193/195 [============================>.] - ETA: 0s - loss: 0.7789 - acc: 0.7311
Epoch 00014: val_loss improved from 1.04561 to 0.98895, saving model to Model.hdf5
195/195 [==============================] - 8s 43ms/step - loss: 0.7789 - acc: 0.7309 - val_loss: 0.9890 - val_acc: 0.6664
Epoch 15/30
193/195 [============================>.] - ETA: 0s - loss: 0.7569 - acc: 0.7377
Epoch 00015: val_loss did not improve from 0.98895
195/195 [==============================] - 8s 42ms/step - loss: 0.7558 - acc: 0.7384 - val_loss: 1.3017 - val_acc: 0.5964
Epoch 16/30
193/195 [============================>.] - ETA: 0s - loss: 0.7487 - acc: 0.7400
Epoch 00016: val_loss improved from 0.98895 to 0.91447, saving model to Model.hdf5
195/195 [==============================] - 8s 43ms/step - loss: 0.7486 - acc: 0.7401 - val_loss: 0.9145 - val_acc: 0.6939
Epoch 17/30
193/195 [============================>.] - ETA: 0s - loss: 0.6920 - acc: 0.7607
Epoch 00017: val_loss did not improve from 0.91447
195/195 [==============================] - 8s 43ms/step - loss: 0.6913 - acc: 0.7611 - val_loss: 1.0266 - val_acc: 0.6581
Epoch 18/30
193/195 [============================>.] - ETA: 0s - loss: 0.6744 - acc: 0.7665
Epoch 00018: val_loss did not improve from 0.91447
195/195 [==============================] - 8s 42ms/step - loss: 0.6743 - acc: 0.7668 - val_loss: 0.9484 - val_acc: 0.6774
Epoch 19/30
193/195 [============================>.] - ETA: 0s - loss: 0.6785 - acc: 0.7636
Epoch 00019: val_loss improved from 0.91447 to 0.91334, saving model to Model.hdf5
195/195 [==============================] - 8s 43ms/step - loss: 0.6786 - acc: 0.7639 - val_loss: 0.9133 - val_acc: 0.6883
Epoch 20/30
193/195 [============================>.] - ETA: 0s - loss: 0.6744 - acc: 0.7702
Epoch 00020: val_loss improved from 0.91334 to 0.86114, saving model to Model.hdf5
195/195 [==============================] - 8s 43ms/step - loss: 0.6738 - acc: 0.7704 - val_loss: 0.8611 - val_acc: 0.7079
Epoch 21/30
193/195 [============================>.] - ETA: 0s - loss: 0.5998 - acc: 0.7905
Epoch 00021: val_loss did not improve from 0.86114
195/195 [==============================] - 8s 43ms/step - loss: 0.5996 - acc: 0.7905 - val_loss: 0.8893 - val_acc: 0.7021
Epoch 22/30
193/195 [============================>.] - ETA: 0s - loss: 0.6090 - acc: 0.7898
Epoch 00022: val_loss did not improve from 0.86114
195/195 [==============================] - 8s 42ms/step - loss: 0.6096 - acc: 0.7895 - val_loss: 0.8769 - val_acc: 0.7077
Epoch 23/30
193/195 [============================>.] - ETA: 0s - loss: 0.6211 - acc: 0.7850
Epoch 00023: val_loss did not improve from 0.86114
195/195 [==============================] - 8s 42ms/step - loss: 0.6222 - acc: 0.7846 - val_loss: 0.8945 - val_acc: 0.7029
Epoch 24/30
193/195 [============================>.] - ETA: 0s - loss: 0.6217 - acc: 0.7824
Epoch 00024: val_loss improved from 0.86114 to 0.76129, saving model to Model.hdf5
195/195 [==============================] - 8s 43ms/step - loss: 0.6230 - acc: 0.7816 - val_loss: 0.7613 - val_acc: 0.7465
Epoch 25/30
193/195 [============================>.] - ETA: 0s - loss: 0.5445 - acc: 0.8090
Epoch 00025: val_loss did not improve from 0.76129
195/195 [==============================] - 8s 43ms/step - loss: 0.5433 - acc: 0.8095 - val_loss: 0.8083 - val_acc: 0.7303
Epoch 26/30
193/195 [============================>.] - ETA: 0s - loss: 0.5633 - acc: 0.8068
Epoch 00026: val_loss did not improve from 0.76129
195/195 [==============================] - 8s 42ms/step - loss: 0.5637 - acc: 0.8066 - val_loss: 1.1410 - val_acc: 0.6529
Epoch 27/30
193/195 [============================>.] - ETA: 0s - loss: 0.5561 - acc: 0.8066
Epoch 00027: val_loss did not improve from 0.76129
195/195 [==============================] - 8s 42ms/step - loss: 0.5569 - acc: 0.8061 - val_loss: 0.8646 - val_acc: 0.7115
Epoch 28/30
193/195 [============================>.] - ETA: 0s - loss: 0.5496 - acc: 0.8120
Epoch 00028: val_loss did not improve from 0.76129
195/195 [==============================] - 8s 42ms/step - loss: 0.5491 - acc: 0.8123 - val_loss: 0.7685 - val_acc: 0.7449
Epoch 29/30
193/195 [============================>.] - ETA: 0s - loss: 0.4913 - acc: 0.8319
Epoch 00029: val_loss did not improve from 0.76129
195/195 [==============================] - 8s 43ms/step - loss: 0.4924 - acc: 0.8317 - val_loss: 1.3107 - val_acc: 0.6464
Epoch 30/30
193/195 [============================>.] - ETA: 0s - loss: 0.5059 - acc: 0.8225
Epoch 00030: val_loss did not improve from 0.76129
195/195 [==============================] - 8s 42ms/step - loss: 0.5061 - acc: 0.8224 - val_loss: 0.9628 - val_acc: 0.6969
